{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE399 HW4\n",
    "## Ziwen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ZiwenLi0325/EE399.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from scipy import integrate\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "T = 8\n",
    "t = np.arange(0,T+dt,dt)\n",
    "beta = 8/3\n",
    "sigma = 10\n",
    "rho = 28\n",
    "\n",
    "# Define activation functions\n",
    "def logsig(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def radbas(x):\n",
    "    return torch.exp(-torch.pow(x, 2))\n",
    "\n",
    "def purelin(x):\n",
    "    return x\n",
    "\n",
    "# Define the Lorenz system\n",
    "def lorenz_deriv(x_y_z, t0, sigma=sigma, beta=beta, rho=rho):\n",
    "    x, y, z = x_y_z\n",
    "    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = purelin(self.fc1(x))\n",
    "        x = purelin(self.fc2(x))\n",
    "        x = purelin(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)  # add batch normalization\n",
    "        self.dropout = nn.Dropout(0.5)  # dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x.unsqueeze(1))  # add an extra dimension for timesteps\n",
    "        x = self.dropout(x)  # add dropout\n",
    "        x = self.fc(self.bn(x.squeeze(1)))  # apply batch normalization before fc\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_rate=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x.unsqueeze(1))  # add an extra dimension for timesteps\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x.squeeze(1))  # remove the timesteps dimension\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ESN(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size, alpha=0.5):\n",
    "        super(ESN, self).__init__()\n",
    "        self.input_weights = nn.Parameter(torch.randn(reservoir_size, input_size) / np.sqrt(input_size), requires_grad=False)\n",
    "        self.reservoir_weights = nn.Parameter(torch.randn(reservoir_size, reservoir_size) / np.sqrt(reservoir_size), requires_grad=False)\n",
    "        self.output_weights = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "        spectral_radius = np.max(np.abs(np.linalg.eigvals(self.reservoir_weights.detach().numpy())))\n",
    "        self.reservoir_weights.data = self.reservoir_weights.data / spectral_radius * alpha\n",
    "        self.reservoir_size = reservoir_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        reservoir_state = torch.zeros(input.size(0), input.size(1), self.reservoir_size, dtype=torch.float32, device=input.device)\n",
    "        reservoir_state[:, 0, :] = torch.tanh(torch.mm(input[:,0,:], self.input_weights.t()))  # initialize reservoir state at t=0\n",
    "        for t in range(1, input.size(1)):  # start loop from t=1\n",
    "            reservoir_state[:, t, :] = torch.tanh(torch.mm(input[:,t,:], self.input_weights.t()) + torch.mm(reservoir_state[:, t-1, :], self.reservoir_weights.t()))\n",
    "        output = torch.sigmoid(self.output_weights(reservoir_state))  # changed activation function\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train for rho = 10, 28, 40"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For NN : Epoch: 0, Loss: 265.38974\n",
      "For NN : Epoch: 2, Loss: 221.21327\n",
      "For NN : Epoch: 4, Loss: 181.20503\n",
      "For NN : Epoch: 6, Loss: 145.34227\n",
      "For NN : Epoch: 8, Loss: 113.58848\n",
      "For NN : Epoch: 10, Loss: 85.93810\n",
      "For NN : Epoch: 12, Loss: 62.42421\n",
      "For NN : Epoch: 14, Loss: 43.10167\n",
      "For NN : Epoch: 16, Loss: 28.00419\n",
      "For NN : Epoch: 18, Loss: 17.07340\n",
      "For NN : Epoch: 20, Loss: 10.06859\n",
      "For NN : Epoch: 22, Loss: 6.48238\n",
      "For NN : Epoch: 24, Loss: 5.51412\n",
      "For NN : Epoch: 26, Loss: 6.15774\n",
      "For NN : Epoch: 28, Loss: 7.40201\n",
      "For NN : Epoch: 30, Loss: 8.45557\n",
      "For NN : Epoch: 32, Loss: 8.88256\n",
      "For NN : Epoch: 34, Loss: 8.59652\n",
      "For NN : Epoch: 36, Loss: 7.75220\n",
      "For NN : Epoch: 38, Loss: 6.61079\n",
      "For NN : Epoch: 40, Loss: 5.43061\n",
      "For NN : Epoch: 42, Loss: 4.40072\n",
      "For NN : Epoch: 44, Loss: 3.61710\n",
      "For NN : Epoch: 46, Loss: 3.09213\n",
      "For NN : Epoch: 48, Loss: 2.78266\n"
     ]
    }
   ],
   "source": [
    "# Generate training data for rho=10, 28, 40\n",
    "rhos = [10, 28, 40]\n",
    "training_input = []\n",
    "training_output = []\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        training_input.append(x_t[j,:-1,:])\n",
    "        training_output.append(x_t[j,1:,:])\n",
    "\n",
    "training_input = np.vstack(training_input)\n",
    "training_output = np.vstack(training_output)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "training_input_torch = torch.tensor(training_input, dtype=torch.float32)\n",
    "training_output_torch = torch.tensor(training_output, dtype=torch.float32)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model_nn_10 = Net()\n",
    "optimizer = Adam(model_nn_10.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(50):  # 50 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_nn_10(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    if epoch % 2 == 0:\n",
    "        print('For NN : Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForwardNN: rho = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For FeedForwardNN: Epoch: 0, Loss: 328.90268\n",
      "For FeedForwardNN: Epoch: 2, Loss: 304.58813\n",
      "For FeedForwardNN: Epoch: 4, Loss: 282.76456\n",
      "For FeedForwardNN: Epoch: 6, Loss: 263.17981\n",
      "For FeedForwardNN: Epoch: 8, Loss: 245.04854\n",
      "For FeedForwardNN: Epoch: 10, Loss: 227.95808\n",
      "For FeedForwardNN: Epoch: 12, Loss: 211.64238\n",
      "For FeedForwardNN: Epoch: 14, Loss: 195.94267\n",
      "For FeedForwardNN: Epoch: 16, Loss: 180.79012\n",
      "For FeedForwardNN: Epoch: 18, Loss: 166.12222\n",
      "For FeedForwardNN: Epoch: 20, Loss: 151.79556\n",
      "For FeedForwardNN: Epoch: 22, Loss: 137.70491\n",
      "For FeedForwardNN: Epoch: 24, Loss: 123.84919\n",
      "For FeedForwardNN: Epoch: 26, Loss: 110.29974\n",
      "For FeedForwardNN: Epoch: 28, Loss: 97.00536\n",
      "For FeedForwardNN: Epoch: 30, Loss: 84.04802\n",
      "For FeedForwardNN: Epoch: 32, Loss: 71.55025\n",
      "For FeedForwardNN: Epoch: 34, Loss: 59.68688\n",
      "For FeedForwardNN: Epoch: 36, Loss: 48.63945\n",
      "For FeedForwardNN: Epoch: 38, Loss: 38.58652\n",
      "For FeedForwardNN: Epoch: 40, Loss: 29.74803\n",
      "For FeedForwardNN: Epoch: 42, Loss: 22.31814\n",
      "For FeedForwardNN: Epoch: 44, Loss: 16.33809\n",
      "For FeedForwardNN: Epoch: 46, Loss: 11.84701\n",
      "For FeedForwardNN: Epoch: 48, Loss: 8.83200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model and optimizer\n",
    "model_FFNN_10 = FeedForwardNN()\n",
    "optimizer = Adam(model_FFNN_10.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(50):  # 100 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_FFNN_10(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    if epoch % 2 == 0:\n",
    "        print('For FeedForwardNN: Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNN: rho = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SimpleRNN: Epoch: 0, Loss: 294.86755\n",
      "For SimpleRNN: Epoch: 2, Loss: 239.81398\n",
      "For SimpleRNN: Epoch: 4, Loss: 187.93257\n",
      "For SimpleRNN: Epoch: 6, Loss: 150.30766\n",
      "For SimpleRNN: Epoch: 8, Loss: 120.57673\n",
      "For SimpleRNN: Epoch: 10, Loss: 100.68346\n",
      "For SimpleRNN: Epoch: 12, Loss: 63.49220\n",
      "For SimpleRNN: Epoch: 14, Loss: 53.25209\n",
      "For SimpleRNN: Epoch: 16, Loss: 39.12949\n",
      "For SimpleRNN: Epoch: 18, Loss: 34.01862\n",
      "For SimpleRNN: Epoch: 20, Loss: 33.48181\n",
      "For SimpleRNN: Epoch: 22, Loss: 22.82074\n",
      "For SimpleRNN: Epoch: 24, Loss: 22.65281\n",
      "For SimpleRNN: Epoch: 26, Loss: 22.64660\n",
      "For SimpleRNN: Epoch: 28, Loss: 23.14999\n",
      "For SimpleRNN: Epoch: 30, Loss: 24.63207\n",
      "For SimpleRNN: Epoch: 32, Loss: 22.14604\n",
      "For SimpleRNN: Epoch: 34, Loss: 23.61559\n",
      "For SimpleRNN: Epoch: 36, Loss: 23.84859\n",
      "For SimpleRNN: Epoch: 38, Loss: 25.66619\n",
      "For SimpleRNN: Epoch: 40, Loss: 23.56945\n",
      "For SimpleRNN: Epoch: 42, Loss: 21.98042\n",
      "For SimpleRNN: Epoch: 44, Loss: 21.90574\n",
      "For SimpleRNN: Epoch: 46, Loss: 21.89048\n",
      "For SimpleRNN: Epoch: 48, Loss: 23.43219\n"
     ]
    }
   ],
   "source": [
    "# Generate training data for rho=10, 28, 40\n",
    "training_input = []\n",
    "training_output = []\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "model_rnn = SimpleRNN(3, 50, 3)  # input size = 3, hidden size = 50, output size = 3\n",
    "optimizer = Adam(model_rnn.parameters(), lr=0.01)  # adjust the learning rate if necessary\n",
    "\n",
    "model_rnn = SimpleRNN(3, 50, 3)  # input size = 3, hidden size = 50, output size = 3\n",
    "optimizer = torch.optim.RMSprop(model_rnn.parameters(), lr=0.01)  # try RMSprop optimizer\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "best_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(50):  # 30 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_rnn(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "\n",
    "    # Add L2 regularization\n",
    "    l2_reg = None\n",
    "    for W in model_rnn.parameters():\n",
    "        if l2_reg is None:\n",
    "            l2_reg = W.norm(2)\n",
    "        else:\n",
    "            l2_reg = l2_reg + W.norm(2)\n",
    "    loss += 0.01 * l2_reg\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model_rnn.parameters(), max_norm=1)\n",
    "\n",
    "    optimizer.step()    # Does the update\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping\n",
    "    if loss.item() < best_loss:\n",
    "        epochs_no_improve = 0\n",
    "        best_loss = loss.item()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        # Check early stopping condition\n",
    "        if epochs_no_improve == 5:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print('For SimpleRNN: Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LSTM : Epoch: 0, Loss: 293.70132\n",
      "For LSTM : Epoch: 1, Loss: 241.11096\n",
      "For LSTM : Epoch: 2, Loss: 180.30966\n",
      "For LSTM : Epoch: 3, Loss: 143.97894\n",
      "For LSTM : Epoch: 4, Loss: 115.30595\n",
      "For LSTM : Epoch: 5, Loss: 103.44151\n",
      "For LSTM : Epoch: 6, Loss: 91.05454\n",
      "For LSTM : Epoch: 7, Loss: 78.71951\n",
      "For LSTM : Epoch: 8, Loss: 78.42130\n",
      "For LSTM : Epoch: 9, Loss: 74.51981\n",
      "For LSTM : Epoch: 10, Loss: 87.20402\n",
      "For LSTM : Epoch: 11, Loss: 76.40270\n",
      "For LSTM : Epoch: 12, Loss: 71.33307\n",
      "For LSTM : Epoch: 13, Loss: 68.19150\n",
      "For LSTM : Epoch: 14, Loss: 66.08487\n",
      "For LSTM : Epoch: 15, Loss: 64.27159\n",
      "For LSTM : Epoch: 16, Loss: 63.15765\n",
      "For LSTM : Epoch: 17, Loss: 62.03322\n",
      "For LSTM : Epoch: 18, Loss: 61.33126\n",
      "For LSTM : Epoch: 19, Loss: 60.37132\n",
      "For LSTM : Epoch: 20, Loss: 59.65639\n",
      "For LSTM : Epoch: 21, Loss: 59.62489\n",
      "For LSTM : Epoch: 22, Loss: 59.54759\n",
      "For LSTM : Epoch: 23, Loss: 59.51046\n",
      "For LSTM : Epoch: 24, Loss: 59.44521\n",
      "For LSTM : Epoch: 25, Loss: 59.39427\n",
      "For LSTM : Epoch: 26, Loss: 59.41616\n",
      "For LSTM : Epoch: 27, Loss: 59.20168\n",
      "For LSTM : Epoch: 28, Loss: 59.20173\n",
      "For LSTM : Epoch: 29, Loss: 59.12910\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and optimizer\n",
    "model_LSTM = LSTM(3, 100, 3)  # input size = 3, hidden size = 100, output size = 3\n",
    "optimizer = torch.optim.RMSprop(model_LSTM.parameters(), lr=0.01)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(30):  # 30 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_LSTM(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print('For LSTM : Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ESN : Epoch: 0, Loss: 293.55878\n",
      "For ESN : Epoch: 1, Loss: 180.07312\n",
      "For ESN : Epoch: 2, Loss: 128.10544\n",
      "For ESN : Epoch: 3, Loss: 99.24440\n",
      "For ESN : Epoch: 4, Loss: 83.95939\n",
      "For ESN : Epoch: 5, Loss: 74.97944\n",
      "For ESN : Epoch: 6, Loss: 69.38313\n",
      "For ESN : Epoch: 7, Loss: 65.77721\n",
      "For ESN : Epoch: 8, Loss: 63.39189\n",
      "For ESN : Epoch: 9, Loss: 61.77143\n",
      "For ESN : Epoch: 10, Loss: 60.63675\n",
      "For ESN : Epoch: 11, Loss: 59.81379\n",
      "For ESN : Epoch: 12, Loss: 59.19267\n",
      "For ESN : Epoch: 13, Loss: 58.70356\n",
      "For ESN : Epoch: 14, Loss: 58.30184\n",
      "For ESN : Epoch: 15, Loss: 57.95888\n",
      "For ESN : Epoch: 16, Loss: 57.65624\n",
      "For ESN : Epoch: 17, Loss: 57.38194\n",
      "For ESN : Epoch: 18, Loss: 57.12825\n",
      "For ESN : Epoch: 19, Loss: 56.89003\n",
      "For ESN : Epoch: 20, Loss: 56.66390\n",
      "For ESN : Epoch: 21, Loss: 56.44754\n",
      "For ESN : Epoch: 22, Loss: 56.23936\n",
      "For ESN : Epoch: 23, Loss: 56.03824\n",
      "For ESN : Epoch: 24, Loss: 55.84336\n",
      "For ESN : Epoch: 25, Loss: 55.65408\n",
      "For ESN : Epoch: 26, Loss: 55.46994\n",
      "For ESN : Epoch: 27, Loss: 55.29048\n",
      "For ESN : Epoch: 28, Loss: 55.11543\n",
      "For ESN : Epoch: 29, Loss: 54.94445\n"
     ]
    }
   ],
   "source": [
    "# Add an extra dimension for time step to the input tensor\n",
    "training_input_torch_time = training_input_torch.unsqueeze(1)\n",
    "\n",
    "# Define the models\n",
    "model_esn_10 = ESN(3, 100, 3)  # increased reservoir size\n",
    "\n",
    "# Initialize the optimaizer for ESN\n",
    "optimizer_esn = torch.optim.Adam(model_esn_10.output_weights.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize the optimizer for ESN\n",
    "optimizer_esn = torch.optim.RMSprop(model_esn_10.output_weights.parameters(), lr=0.01)  # changed optimizer\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the ESN network\n",
    "best_loss_esn = np.inf\n",
    "epochs_no_improve_esn = 0\n",
    "\n",
    "for epoch in range(30):  # match the number of epochs with NN\n",
    "    optimizer_esn.zero_grad()   # zero the gradient buffers\n",
    "    output_esn = model_esn_10(training_input_torch_time)\n",
    "    # Remove the time step dimension from the output for loss calculation\n",
    "    output_esn = output_esn.squeeze(1)\n",
    "    loss_esn = criterion(output_esn, training_output_torch)\n",
    "\n",
    "    # Add L2 regularization\n",
    "    l2_reg = None\n",
    "    for W in model_esn_10.output_weights.parameters():\n",
    "        if l2_reg is None:\n",
    "            l2_reg = W.norm(2)\n",
    "        else:\n",
    "            l2_reg = l2_reg + W.norm(2)\n",
    "    loss_esn += 0.01 * l2_reg  # L2 regularization\n",
    "\n",
    "    loss_esn.backward()\n",
    "    optimizer_esn.step()  # Does the update\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_esn.item() < best_loss_esn:\n",
    "        epochs_no_improve_esn = 0\n",
    "        best_loss_esn = loss_esn.item()\n",
    "    else:\n",
    "        epochs_no_improve_esn += 1\n",
    "        # Check early stopping condition\n",
    "        if epochs_no_improve_esn == 5:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "    print('For ESN : Epoch: {}, Loss: {:.5f}'.format(epoch, loss_esn.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forecasting dynamics for rho = 17:\n",
      "Mean Squared Error for model Net: 0.00046261821989901364\n",
      "Mean Squared Error for model FeedForwardNN: 7.882964382588398e-06\n",
      "Mean Squared Error for model LSTM: 0.0024459792766720057\n",
      "Mean Squared Error for model SimpleRNN: 0.018040049821138382\n",
      "\n",
      "Forecasting dynamics for rho = 35:\n",
      "Mean Squared Error for model Net: 0.3871428668498993\n",
      "Mean Squared Error for model FeedForwardNN: 0.09199709445238113\n",
      "Mean Squared Error for model LSTM: 0.4567260444164276\n",
      "Mean Squared Error for model SimpleRNN: 0.17212016880512238\n"
     ]
    }
   ],
   "source": [
    "rhos = [17, 35]\n",
    "\n",
    "def generate_lorenz_data(rho, initial_state=[1, 1, 1], dt=0.01, N=10000, sigma=10, beta=8/3):\n",
    "    t = np.arange(0, N*dt, dt)\n",
    "    traj = integrate.odeint(lorenz_deriv, initial_state, t, args=(sigma, beta, rho))\n",
    "    return traj[:-1, :], traj[1:, :]\n",
    "\n",
    "for rho in rhos:\n",
    "    print(f\"\\nForecasting dynamics for rho = {rho}:\")\n",
    "\n",
    "    # Generate Lorenz system data for rho\n",
    "    nn_input, nn_output = generate_lorenz_data(rho)\n",
    "\n",
    "    # Train-test split\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(nn_input))\n",
    "    train_input, train_output = nn_input[:split_idx], nn_output[:split_idx]\n",
    "    test_input, test_output = nn_input[split_idx:], nn_output[split_idx:]\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    train_input_torch = torch.from_numpy(train_input.astype(np.float32))\n",
    "    train_output_torch = torch.from_numpy(train_output.astype(np.float32))\n",
    "    test_input_torch = torch.from_numpy(test_input.astype(np.float32))\n",
    "    test_output_torch = torch.from_numpy(test_output.astype(np.float32))\n",
    "\n",
    "    # Define the models\n",
    "    model1 = model_nn_10\n",
    "    model2 = model_FFNN_10\n",
    "    model3 = model_LSTM\n",
    "    model4 = model_rnn\n",
    "    model5 = model_esn_10\n",
    "  \n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train each model and make predictions\n",
    "    for model in [model1, model2, model3, model4]:\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_input_torch)\n",
    "            loss = criterion(output, train_output_torch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(test_input_torch)\n",
    "\n",
    "        # Compute the mean squared error of the predictions\n",
    "        mse = mean_squared_error(test_output_torch.detach().numpy(), predictions.detach().numpy())\n",
    "        print(f\"Mean Squared Error for model {model.__class__.__name__}: {mse}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forecasting dynamics for rho = 17:\n",
      "Mean Squared Error for model ESN: 0.0032536678481847048\n",
      "\n",
      "Forecasting dynamics for rho = 35:\n",
      "Mean Squared Error for model ESN: 23.312599182128906\n"
     ]
    }
   ],
   "source": [
    "for rho in rhos:\n",
    "    print(f\"\\nForecasting dynamics for rho = {rho}:\")\n",
    "\n",
    "    # Generate Lorenz system data for rho\n",
    "    nn_input, nn_output = generate_lorenz_data(rho)\n",
    "\n",
    "    # Train-test split\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(nn_input))\n",
    "    train_input, train_output = nn_input[:split_idx], nn_output[:split_idx]\n",
    "    test_input, test_output = nn_input[split_idx:], nn_output[split_idx:]\n",
    "\n",
    "    # Convert to torch tensors and add time dimension\n",
    "    train_input_torch = torch.from_numpy(train_input.astype(np.float32)).unsqueeze(1)\n",
    "    train_output_torch = torch.from_numpy(train_output.astype(np.float32))\n",
    "    test_input_torch = torch.from_numpy(test_input.astype(np.float32)).unsqueeze(1)\n",
    "    test_output_torch = torch.from_numpy(test_output.astype(np.float32))\n",
    "\n",
    "    # Define the models\n",
    "    model5 = model_esn_10\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train each model and make predictions\n",
    "    for model in [model5]:\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_input_torch)\n",
    "            output = output.squeeze(1)  # remove the time dimension for ESN output\n",
    "            loss = criterion(output, train_output_torch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(test_input_torch)\n",
    "            predictions = predictions.squeeze(1)  # remove the time dimension for ESN predictions\n",
    "\n",
    "        # Compute the mean squared error of the predictions\n",
    "        mse = mean_squared_error(test_output_torch.detach().numpy(), predictions.detach().numpy())\n",
    "        print(f\"Mean Squared Error for model {model.__class__.__name__}: {mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
