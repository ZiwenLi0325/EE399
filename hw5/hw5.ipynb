{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE399 HW5\n",
    "## Ziwen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ZiwenLi0325/EE399.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from scipy import integrate\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "T = 8\n",
    "t = np.arange(0,T+dt,dt)\n",
    "beta = 8/3\n",
    "sigma = 10\n",
    "rho = 28\n",
    "\n",
    "# Define activation functions\n",
    "def logsig(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def radbas(x):\n",
    "    return torch.exp(-torch.pow(x, 2))\n",
    "\n",
    "def purelin(x):\n",
    "    return x\n",
    "\n",
    "# Define the Lorenz system\n",
    "def lorenz_deriv(x_y_z, t0, sigma=sigma, beta=beta, rho=rho):\n",
    "    x, y, z = x_y_z\n",
    "    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = purelin(self.fc1(x))\n",
    "        x = purelin(self.fc2(x))\n",
    "        x = purelin(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)  # add batch normalization\n",
    "        self.dropout = nn.Dropout(0.5)  # dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x.unsqueeze(1))  # add an extra dimension for timesteps\n",
    "        x = self.dropout(x)  # add dropout\n",
    "        x = self.fc(self.bn(x.squeeze(1)))  # apply batch normalization before fc\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_rate=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x.unsqueeze(1))  # add an extra dimension for timesteps\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x.squeeze(1))  # remove the timesteps dimension\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ESN(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size, alpha=0.5):\n",
    "        super(ESN, self).__init__()\n",
    "        self.input_weights = nn.Parameter(torch.randn(reservoir_size, input_size) / np.sqrt(input_size), requires_grad=False)\n",
    "        self.reservoir_weights = nn.Parameter(torch.randn(reservoir_size, reservoir_size) / np.sqrt(reservoir_size), requires_grad=False)\n",
    "        self.output_weights = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "        spectral_radius = np.max(np.abs(np.linalg.eigvals(self.reservoir_weights.detach().numpy())))\n",
    "        self.reservoir_weights.data = self.reservoir_weights.data / spectral_radius * alpha\n",
    "        self.reservoir_size = reservoir_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        reservoir_state = torch.zeros(input.size(0), input.size(1), self.reservoir_size, dtype=torch.float32, device=input.device)\n",
    "        reservoir_state[:, 0, :] = torch.tanh(torch.mm(input[:,0,:], self.input_weights.t()))  # initialize reservoir state at t=0\n",
    "        for t in range(1, input.size(1)):  # start loop from t=1\n",
    "            reservoir_state[:, t, :] = torch.tanh(torch.mm(input[:,t,:], self.input_weights.t()) + torch.mm(reservoir_state[:, t-1, :], self.reservoir_weights.t()))\n",
    "        output = torch.sigmoid(self.output_weights(reservoir_state))  # changed activation function\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train for rho = 10, 28, 40"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For NN : Epoch: 0, Loss: 277.21008\n",
      "For NN : Epoch: 2, Loss: 217.84122\n",
      "For NN : Epoch: 4, Loss: 166.72914\n",
      "For NN : Epoch: 6, Loss: 123.80717\n",
      "For NN : Epoch: 8, Loss: 89.08292\n",
      "For NN : Epoch: 10, Loss: 62.46132\n",
      "For NN : Epoch: 12, Loss: 43.60970\n",
      "For NN : Epoch: 14, Loss: 31.80769\n",
      "For NN : Epoch: 16, Loss: 25.79365\n",
      "For NN : Epoch: 18, Loss: 23.71923\n",
      "For NN : Epoch: 20, Loss: 23.38392\n",
      "For NN : Epoch: 22, Loss: 22.82800\n",
      "For NN : Epoch: 24, Loss: 20.96345\n",
      "For NN : Epoch: 26, Loss: 17.73151\n",
      "For NN : Epoch: 28, Loss: 13.76037\n",
      "For NN : Epoch: 30, Loss: 9.87717\n",
      "For NN : Epoch: 32, Loss: 6.74112\n",
      "For NN : Epoch: 34, Loss: 4.67531\n",
      "For NN : Epoch: 36, Loss: 3.66657\n",
      "For NN : Epoch: 38, Loss: 3.46505\n",
      "For NN : Epoch: 40, Loss: 3.71740\n",
      "For NN : Epoch: 42, Loss: 4.08684\n",
      "For NN : Epoch: 44, Loss: 4.33377\n",
      "For NN : Epoch: 46, Loss: 4.34769\n",
      "For NN : Epoch: 48, Loss: 4.13516\n"
     ]
    }
   ],
   "source": [
    "# Generate training data for rho=10, 28, 40\n",
    "rhos = [10, 28, 40]\n",
    "training_input = []\n",
    "training_output = []\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        training_input.append(x_t[j,:-1,:])\n",
    "        training_output.append(x_t[j,1:,:])\n",
    "\n",
    "training_input = np.vstack(training_input)\n",
    "training_output = np.vstack(training_output)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "training_input_torch = torch.tensor(training_input, dtype=torch.float32)\n",
    "training_output_torch = torch.tensor(training_output, dtype=torch.float32)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model_nn_10 = Net()\n",
    "optimizer = Adam(model_nn_10.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(50):  # 50 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_nn_10(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    if epoch % 2 == 0:\n",
    "        print('For NN : Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForwardNN: rho = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For FeedForwardNN: Epoch: 0, Loss: 316.87064\n",
      "For FeedForwardNN: Epoch: 2, Loss: 294.00208\n",
      "For FeedForwardNN: Epoch: 4, Loss: 272.27576\n",
      "For FeedForwardNN: Epoch: 6, Loss: 251.90163\n",
      "For FeedForwardNN: Epoch: 8, Loss: 232.58051\n",
      "For FeedForwardNN: Epoch: 10, Loss: 214.03636\n",
      "For FeedForwardNN: Epoch: 12, Loss: 196.16214\n",
      "For FeedForwardNN: Epoch: 14, Loss: 178.97263\n",
      "For FeedForwardNN: Epoch: 16, Loss: 162.35989\n",
      "For FeedForwardNN: Epoch: 18, Loss: 146.13577\n",
      "For FeedForwardNN: Epoch: 20, Loss: 130.19473\n",
      "For FeedForwardNN: Epoch: 22, Loss: 114.53085\n",
      "For FeedForwardNN: Epoch: 24, Loss: 99.22081\n",
      "For FeedForwardNN: Epoch: 26, Loss: 84.40943\n",
      "For FeedForwardNN: Epoch: 28, Loss: 70.35100\n",
      "For FeedForwardNN: Epoch: 30, Loss: 57.35516\n",
      "For FeedForwardNN: Epoch: 32, Loss: 45.51680\n",
      "For FeedForwardNN: Epoch: 34, Loss: 34.97275\n",
      "For FeedForwardNN: Epoch: 36, Loss: 25.89125\n",
      "For FeedForwardNN: Epoch: 38, Loss: 18.42566\n",
      "For FeedForwardNN: Epoch: 40, Loss: 12.65402\n",
      "For FeedForwardNN: Epoch: 42, Loss: 8.54422\n",
      "For FeedForwardNN: Epoch: 44, Loss: 5.95206\n",
      "For FeedForwardNN: Epoch: 46, Loss: 4.62157\n",
      "For FeedForwardNN: Epoch: 48, Loss: 4.21154\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model and optimizer\n",
    "model_FFNN_10 = FeedForwardNN()\n",
    "optimizer = Adam(model_FFNN_10.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(50):  # 100 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_FFNN_10(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    if epoch % 2 == 0:\n",
    "        print('For FeedForwardNN: Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNN: rho = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZiwenLi\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SimpleRNN: Epoch: 0, Loss: 293.63480\n",
      "For SimpleRNN: Epoch: 2, Loss: 232.62386\n",
      "For SimpleRNN: Epoch: 4, Loss: 182.26834\n",
      "For SimpleRNN: Epoch: 6, Loss: 141.85216\n",
      "For SimpleRNN: Epoch: 8, Loss: 116.19038\n",
      "For SimpleRNN: Epoch: 10, Loss: 94.62010\n",
      "For SimpleRNN: Epoch: 12, Loss: 75.66857\n",
      "For SimpleRNN: Epoch: 14, Loss: 56.39511\n",
      "For SimpleRNN: Epoch: 16, Loss: 40.04153\n",
      "For SimpleRNN: Epoch: 18, Loss: 33.05820\n",
      "For SimpleRNN: Epoch: 20, Loss: 29.68453\n",
      "For SimpleRNN: Epoch: 22, Loss: 28.07904\n",
      "For SimpleRNN: Epoch: 24, Loss: 23.15183\n",
      "For SimpleRNN: Epoch: 26, Loss: 23.79162\n",
      "For SimpleRNN: Epoch: 28, Loss: 24.65107\n",
      "For SimpleRNN: Epoch: 30, Loss: 21.97364\n",
      "For SimpleRNN: Epoch: 32, Loss: 22.37099\n",
      "For SimpleRNN: Epoch: 34, Loss: 19.13055\n",
      "For SimpleRNN: Epoch: 36, Loss: 18.09512\n",
      "For SimpleRNN: Epoch: 38, Loss: 17.62892\n",
      "For SimpleRNN: Epoch: 40, Loss: 16.92105\n",
      "For SimpleRNN: Epoch: 42, Loss: 16.22765\n",
      "For SimpleRNN: Epoch: 44, Loss: 15.63083\n",
      "For SimpleRNN: Epoch: 46, Loss: 14.85840\n",
      "For SimpleRNN: Epoch: 48, Loss: 14.37607\n"
     ]
    }
   ],
   "source": [
    "# Generate training data for rho=10, 28, 40\n",
    "training_input = []\n",
    "training_output = []\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "model_rnn = SimpleRNN(3, 50, 3)  # input size = 3, hidden size = 50, output size = 3\n",
    "optimizer = Adam(model_rnn.parameters(), lr=0.01)  # adjust the learning rate if necessary\n",
    "\n",
    "model_rnn = SimpleRNN(3, 50, 3)  # input size = 3, hidden size = 50, output size = 3\n",
    "optimizer = torch.optim.RMSprop(model_rnn.parameters(), lr=0.01)  # try RMSprop optimizer\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "best_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(50):  # 30 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_rnn(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "\n",
    "    # Add L2 regularization\n",
    "    l2_reg = None\n",
    "    for W in model_rnn.parameters():\n",
    "        if l2_reg is None:\n",
    "            l2_reg = W.norm(2)\n",
    "        else:\n",
    "            l2_reg = l2_reg + W.norm(2)\n",
    "    loss += 0.01 * l2_reg\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model_rnn.parameters(), max_norm=1)\n",
    "\n",
    "    optimizer.step()    # Does the update\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping\n",
    "    if loss.item() < best_loss:\n",
    "        epochs_no_improve = 0\n",
    "        best_loss = loss.item()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        # Check early stopping condition\n",
    "        if epochs_no_improve == 5:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print('For SimpleRNN: Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LSTM : Epoch: 0, Loss: 291.80551\n",
      "For LSTM : Epoch: 1, Loss: 238.69255\n",
      "For LSTM : Epoch: 2, Loss: 176.36925\n",
      "For LSTM : Epoch: 3, Loss: 144.74005\n",
      "For LSTM : Epoch: 4, Loss: 123.51939\n",
      "For LSTM : Epoch: 5, Loss: 104.93462\n",
      "For LSTM : Epoch: 6, Loss: 92.71381\n",
      "For LSTM : Epoch: 7, Loss: 80.65710\n",
      "For LSTM : Epoch: 8, Loss: 86.83304\n",
      "For LSTM : Epoch: 9, Loss: 79.17844\n",
      "For LSTM : Epoch: 10, Loss: 70.11256\n",
      "For LSTM : Epoch: 11, Loss: 67.81390\n",
      "For LSTM : Epoch: 12, Loss: 66.13931\n",
      "For LSTM : Epoch: 13, Loss: 64.66452\n",
      "For LSTM : Epoch: 14, Loss: 63.53388\n",
      "For LSTM : Epoch: 15, Loss: 62.44979\n",
      "For LSTM : Epoch: 16, Loss: 61.57392\n",
      "For LSTM : Epoch: 17, Loss: 60.65466\n",
      "For LSTM : Epoch: 18, Loss: 59.68523\n",
      "For LSTM : Epoch: 19, Loss: 58.92725\n",
      "For LSTM : Epoch: 20, Loss: 58.09414\n",
      "For LSTM : Epoch: 21, Loss: 57.93455\n",
      "For LSTM : Epoch: 22, Loss: 58.04333\n",
      "For LSTM : Epoch: 23, Loss: 57.89446\n",
      "For LSTM : Epoch: 24, Loss: 57.78593\n",
      "For LSTM : Epoch: 25, Loss: 57.77925\n",
      "For LSTM : Epoch: 26, Loss: 57.54692\n",
      "For LSTM : Epoch: 27, Loss: 57.50652\n",
      "For LSTM : Epoch: 28, Loss: 57.47475\n",
      "For LSTM : Epoch: 29, Loss: 57.50306\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and optimizer\n",
    "model_LSTM = LSTM(3, 100, 3)  # input size = 3, hidden size = 100, output size = 3\n",
    "optimizer = torch.optim.RMSprop(model_LSTM.parameters(), lr=0.01)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(30):  # 30 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_LSTM(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print('For LSTM : Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ESN : Epoch: 0, Loss: 285.54865\n",
      "For ESN : Epoch: 1, Loss: 275.62036\n",
      "For ESN : Epoch: 2, Loss: 274.38834\n",
      "For ESN : Epoch: 3, Loss: 273.57028\n",
      "For ESN : Epoch: 4, Loss: 273.41779\n",
      "For ESN : Epoch: 5, Loss: 273.36407\n",
      "For ESN : Epoch: 6, Loss: 273.34329\n",
      "For ESN : Epoch: 7, Loss: 273.32999\n",
      "For ESN : Epoch: 8, Loss: 273.31857\n",
      "For ESN : Epoch: 9, Loss: 273.30835\n",
      "For ESN : Epoch: 10, Loss: 273.29919\n",
      "For ESN : Epoch: 11, Loss: 273.29083\n",
      "For ESN : Epoch: 12, Loss: 273.28326\n",
      "For ESN : Epoch: 13, Loss: 273.27634\n",
      "For ESN : Epoch: 14, Loss: 273.26996\n",
      "For ESN : Epoch: 15, Loss: 273.26401\n",
      "For ESN : Epoch: 16, Loss: 273.25854\n",
      "For ESN : Epoch: 17, Loss: 273.25345\n",
      "For ESN : Epoch: 18, Loss: 273.24869\n",
      "For ESN : Epoch: 19, Loss: 273.24423\n",
      "For ESN : Epoch: 20, Loss: 273.24005\n",
      "For ESN : Epoch: 21, Loss: 273.23608\n",
      "For ESN : Epoch: 22, Loss: 273.23239\n",
      "For ESN : Epoch: 23, Loss: 273.22885\n",
      "For ESN : Epoch: 24, Loss: 273.22556\n",
      "For ESN : Epoch: 25, Loss: 273.22238\n",
      "For ESN : Epoch: 26, Loss: 273.21939\n",
      "For ESN : Epoch: 27, Loss: 273.21658\n",
      "For ESN : Epoch: 28, Loss: 273.21387\n",
      "For ESN : Epoch: 29, Loss: 273.21124\n"
     ]
    }
   ],
   "source": [
    "# Add an extra dimension for time step to the input tensor\n",
    "training_input_torch_time = training_input_torch.unsqueeze(1)\n",
    "\n",
    "# Define the models\n",
    "model_esn_10 = ESN(3, 100, 3)  # increased reservoir size\n",
    "\n",
    "# Initialize the optimaizer for ESN\n",
    "optimizer_esn = torch.optim.Adam(model_esn_10.output_weights.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize the optimizer for ESN\n",
    "optimizer_esn = torch.optim.RMSprop(model_esn_10.output_weights.parameters(), lr=0.01)  # changed optimizer\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the ESN network\n",
    "best_loss_esn = np.inf\n",
    "epochs_no_improve_esn = 0\n",
    "\n",
    "for epoch in range(30):  # match the number of epochs with NN\n",
    "    optimizer_esn.zero_grad()   # zero the gradient buffers\n",
    "    output_esn = model_esn_10(training_input_torch_time)\n",
    "    # Remove the time step dimension from the output for loss calculation\n",
    "    output_esn = output_esn.squeeze(1)\n",
    "    loss_esn = criterion(output_esn, training_output_torch)\n",
    "\n",
    "    # Add L2 regularization\n",
    "    l2_reg = None\n",
    "    for W in model_esn_10.output_weights.parameters():\n",
    "        if l2_reg is None:\n",
    "            l2_reg = W.norm(2)\n",
    "        else:\n",
    "            l2_reg = l2_reg + W.norm(2)\n",
    "    loss_esn += 0.01 * l2_reg  # L2 regularization\n",
    "\n",
    "    loss_esn.backward()\n",
    "    optimizer_esn.step()  # Does the update\n",
    "\n",
    "    # Early stopping\n",
    "    if loss_esn.item() < best_loss_esn:\n",
    "        epochs_no_improve_esn = 0\n",
    "        best_loss_esn = loss_esn.item()\n",
    "    else:\n",
    "        epochs_no_improve_esn += 1\n",
    "        # Check early stopping condition\n",
    "        if epochs_no_improve_esn == 5:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "    print('For ESN : Epoch: {}, Loss: {:.5f}'.format(epoch, loss_esn.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forecasting dynamics for rho = 17:\n",
      "Mean Squared Error for model Net: 0.0003400488931220025\n",
      "Mean Squared Error for model FeedForwardNN: 0.00012982710904907435\n",
      "Mean Squared Error for model LSTM: 0.005836538504809141\n",
      "Mean Squared Error for model SimpleRNN: 0.06037695333361626\n",
      "\n",
      "Forecasting dynamics for rho = 35:\n",
      "Mean Squared Error for model Net: 0.39486798644065857\n",
      "Mean Squared Error for model FeedForwardNN: 0.10435580462217331\n",
      "Mean Squared Error for model LSTM: 0.4556368291378021\n",
      "Mean Squared Error for model SimpleRNN: 0.239154651761055\n"
     ]
    }
   ],
   "source": [
    "rhos = [17, 35]\n",
    "\n",
    "def generate_lorenz_data(rho, initial_state=[1, 1, 1], dt=0.01, N=10000, sigma=10, beta=8/3):\n",
    "    t = np.arange(0, N*dt, dt)\n",
    "    traj = integrate.odeint(lorenz_deriv, initial_state, t, args=(sigma, beta, rho))\n",
    "    return traj[:-1, :], traj[1:, :]\n",
    "\n",
    "for rho in rhos:\n",
    "    print(f\"\\nForecasting dynamics for rho = {rho}:\")\n",
    "\n",
    "    # Generate Lorenz system data for rho\n",
    "    nn_input, nn_output = generate_lorenz_data(rho)\n",
    "\n",
    "    # Train-test split\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(nn_input))\n",
    "    train_input, train_output = nn_input[:split_idx], nn_output[:split_idx]\n",
    "    test_input, test_output = nn_input[split_idx:], nn_output[split_idx:]\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    train_input_torch = torch.from_numpy(train_input.astype(np.float32))\n",
    "    train_output_torch = torch.from_numpy(train_output.astype(np.float32))\n",
    "    test_input_torch = torch.from_numpy(test_input.astype(np.float32))\n",
    "    test_output_torch = torch.from_numpy(test_output.astype(np.float32))\n",
    "\n",
    "    # Define the models\n",
    "    model1 = model_nn_10\n",
    "    model2 = model_FFNN_10\n",
    "    model3 = model_LSTM\n",
    "    model4 = model_rnn\n",
    "    model5 = model_esn_10\n",
    "  \n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train each model and make predictions\n",
    "    for model in [model1, model2, model3, model4]:\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_input_torch)\n",
    "            loss = criterion(output, train_output_torch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(test_input_torch)\n",
    "\n",
    "        # Compute the mean squared error of the predictions\n",
    "        mse = mean_squared_error(test_output_torch.detach().numpy(), predictions.detach().numpy())\n",
    "        print(f\"Mean Squared Error for model {model.__class__.__name__}: {mse}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forecasting dynamics for rho = 17:\n",
      "Mean Squared Error for model ESN: 103.44430541992188\n",
      "\n",
      "Forecasting dynamics for rho = 35:\n",
      "Mean Squared Error for model ESN: 384.436767578125\n"
     ]
    }
   ],
   "source": [
    "for rho in rhos:\n",
    "    print(f\"\\nForecasting dynamics for rho = {rho}:\")\n",
    "\n",
    "    # Generate Lorenz system data for rho\n",
    "    nn_input, nn_output = generate_lorenz_data(rho)\n",
    "\n",
    "    # Train-test split\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(nn_input))\n",
    "    train_input, train_output = nn_input[:split_idx], nn_output[:split_idx]\n",
    "    test_input, test_output = nn_input[split_idx:], nn_output[split_idx:]\n",
    "\n",
    "    # Convert to torch tensors and add time dimension\n",
    "    train_input_torch = torch.from_numpy(train_input.astype(np.float32)).unsqueeze(1)\n",
    "    train_output_torch = torch.from_numpy(train_output.astype(np.float32))\n",
    "    test_input_torch = torch.from_numpy(test_input.astype(np.float32)).unsqueeze(1)\n",
    "    test_output_torch = torch.from_numpy(test_output.astype(np.float32))\n",
    "\n",
    "    # Define the models\n",
    "    model5 = model_esn_10\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train each model and make predictions\n",
    "    for model in [model5]:\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_input_torch)\n",
    "            output = output.squeeze(1)  # remove the time dimension for ESN output\n",
    "            loss = criterion(output, train_output_torch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(test_input_torch)\n",
    "            predictions = predictions.squeeze(1)  # remove the time dimension for ESN predictions\n",
    "\n",
    "        # Compute the mean squared error of the predictions\n",
    "        mse = mean_squared_error(test_output_torch.detach().numpy(), predictions.detach().numpy())\n",
    "        print(f\"Mean Squared Error for model {model.__class__.__name__}: {mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
