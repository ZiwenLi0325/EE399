{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE399 HW4\n",
    "## Ziwen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ZiwenLi0325/EE399.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from scipy import integrate\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "T = 8\n",
    "t = np.arange(0,T+dt,dt)\n",
    "beta = 8/3\n",
    "sigma = 10\n",
    "rho = 28\n",
    "\n",
    "# Define activation functions\n",
    "def logsig(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def radbas(x):\n",
    "    return torch.exp(-torch.pow(x, 2))\n",
    "\n",
    "def purelin(x):\n",
    "    return x\n",
    "\n",
    "# Define the Lorenz system\n",
    "def lorenz_deriv(x_y_z, t0, sigma=sigma, beta=beta, rho=rho):\n",
    "    x, y, z = x_y_z\n",
    "    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = purelin(self.fc1(x))\n",
    "        x = purelin(self.fc2(x))\n",
    "        x = purelin(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)  # add batch normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x.unsqueeze(1))  # add an extra dimension for timesteps\n",
    "        x = self.fc(self.bn(x.squeeze(1)))  # apply batch normalization before fc\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x.unsqueeze(1))  # add an extra dimension for timesteps\n",
    "        x = self.fc(x.squeeze(1))  # remove the timesteps dimension\n",
    "        return x\n",
    "\n",
    "\n",
    "class ESN(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size, alpha=0.5):\n",
    "        super(ESN, self).__init__()\n",
    "        self.input_weights = nn.Parameter(torch.randn(reservoir_size, input_size) / np.sqrt(input_size), requires_grad=False)\n",
    "        self.reservoir_weights = nn.Parameter(torch.randn(reservoir_size, reservoir_size) / np.sqrt(reservoir_size), requires_grad=False)\n",
    "        self.output_weights = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "        spectral_radius = np.max(np.abs(np.linalg.eigvals(self.reservoir_weights.detach().numpy())))\n",
    "        self.reservoir_weights.data = self.reservoir_weights.data / spectral_radius * alpha\n",
    "        self.reservoir_size = reservoir_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        reservoir_state = torch.zeros(input.size(0), input.size(1), self.reservoir_size, dtype=torch.float32, device=input.device)\n",
    "        reservoir_state[:, 0, :] = torch.tanh(torch.mm(input[:,0,:], self.input_weights.t()))  # initialize reservoir state at t=0\n",
    "        for t in range(1, input.size(1)):  # start loop from t=1\n",
    "            reservoir_state[:, t, :] = torch.tanh(torch.mm(input[:,t,:], self.input_weights.t()) + torch.mm(reservoir_state[:, t-1, :], self.reservoir_weights.t()))\n",
    "        output = self.output_weights(reservoir_state)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train for rho = 10, 28, 40"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For NN : Epoch: 0, Loss: 262.59683\n",
      "For NN : Epoch: 1, Loss: 233.40649\n",
      "For NN : Epoch: 2, Loss: 205.98650\n",
      "For NN : Epoch: 3, Loss: 180.33591\n",
      "For NN : Epoch: 4, Loss: 156.46880\n",
      "For NN : Epoch: 5, Loss: 134.41217\n",
      "For NN : Epoch: 6, Loss: 114.19106\n",
      "For NN : Epoch: 7, Loss: 95.82480\n",
      "For NN : Epoch: 8, Loss: 79.32680\n",
      "For NN : Epoch: 9, Loss: 64.70339\n",
      "For NN : Epoch: 10, Loss: 51.95095\n",
      "For NN : Epoch: 11, Loss: 41.05153\n",
      "For NN : Epoch: 12, Loss: 31.96740\n",
      "For NN : Epoch: 13, Loss: 24.63585\n",
      "For NN : Epoch: 14, Loss: 18.96352\n",
      "For NN : Epoch: 15, Loss: 14.82186\n",
      "For NN : Epoch: 16, Loss: 12.04458\n",
      "For NN : Epoch: 17, Loss: 10.42864\n",
      "For NN : Epoch: 18, Loss: 9.73999\n",
      "For NN : Epoch: 19, Loss: 9.72519\n",
      "For NN : Epoch: 20, Loss: 10.12858\n",
      "For NN : Epoch: 21, Loss: 10.71262\n",
      "For NN : Epoch: 22, Loss: 11.27713\n",
      "For NN : Epoch: 23, Loss: 11.67324\n",
      "For NN : Epoch: 24, Loss: 11.80956\n",
      "For NN : Epoch: 25, Loss: 11.65015\n",
      "For NN : Epoch: 26, Loss: 11.20623\n",
      "For NN : Epoch: 27, Loss: 10.52380\n",
      "For NN : Epoch: 28, Loss: 9.67014\n",
      "For NN : Epoch: 29, Loss: 8.72113\n"
     ]
    }
   ],
   "source": [
    "# Generate training data for rho=10, 28, 40\n",
    "rhos = [10, 28, 40]\n",
    "training_input = []\n",
    "training_output = []\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        training_input.append(x_t[j,:-1,:])\n",
    "        training_output.append(x_t[j,1:,:])\n",
    "\n",
    "training_input = np.vstack(training_input)\n",
    "training_output = np.vstack(training_output)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "training_input_torch = torch.tensor(training_input, dtype=torch.float32)\n",
    "training_output_torch = torch.tensor(training_output, dtype=torch.float32)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model_nn_10 = Net()\n",
    "optimizer = Adam(model_nn_10.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(30):  # 100 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_nn_10(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    print('For NN : Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForwardNN: rho = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For FeedForwardNN: Epoch: 0, Loss: 362.94348\n",
      "For FeedForwardNN: Epoch: 1, Loss: 353.25021\n",
      "For FeedForwardNN: Epoch: 2, Loss: 343.87408\n",
      "For FeedForwardNN: Epoch: 3, Loss: 334.75436\n",
      "For FeedForwardNN: Epoch: 4, Loss: 325.90726\n",
      "For FeedForwardNN: Epoch: 5, Loss: 317.31183\n",
      "For FeedForwardNN: Epoch: 6, Loss: 308.90826\n",
      "For FeedForwardNN: Epoch: 7, Loss: 300.69992\n",
      "For FeedForwardNN: Epoch: 8, Loss: 292.65540\n",
      "For FeedForwardNN: Epoch: 9, Loss: 284.75180\n",
      "For FeedForwardNN: Epoch: 10, Loss: 276.97278\n",
      "For FeedForwardNN: Epoch: 11, Loss: 269.32770\n",
      "For FeedForwardNN: Epoch: 12, Loss: 261.78406\n",
      "For FeedForwardNN: Epoch: 13, Loss: 254.30779\n",
      "For FeedForwardNN: Epoch: 14, Loss: 246.85889\n",
      "For FeedForwardNN: Epoch: 15, Loss: 239.46001\n",
      "For FeedForwardNN: Epoch: 16, Loss: 232.12520\n",
      "For FeedForwardNN: Epoch: 17, Loss: 224.86005\n",
      "For FeedForwardNN: Epoch: 18, Loss: 217.67726\n",
      "For FeedForwardNN: Epoch: 19, Loss: 210.59267\n",
      "For FeedForwardNN: Epoch: 20, Loss: 203.60683\n",
      "For FeedForwardNN: Epoch: 21, Loss: 196.69449\n",
      "For FeedForwardNN: Epoch: 22, Loss: 189.83356\n",
      "For FeedForwardNN: Epoch: 23, Loss: 183.01007\n",
      "For FeedForwardNN: Epoch: 24, Loss: 176.21210\n",
      "For FeedForwardNN: Epoch: 25, Loss: 169.44389\n",
      "For FeedForwardNN: Epoch: 26, Loss: 162.71191\n",
      "For FeedForwardNN: Epoch: 27, Loss: 156.00400\n",
      "For FeedForwardNN: Epoch: 28, Loss: 149.31313\n",
      "For FeedForwardNN: Epoch: 29, Loss: 142.63553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model and optimizer\n",
    "model_FFNN_10 = FeedForwardNN()\n",
    "optimizer = Adam(model_FFNN_10.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(30):  # 100 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_FFNN_10(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    print('For FeedForwardNN: Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNN: rho = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZiwenLi\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SimpleRNN: Epoch: 0, Loss: 552.62665\n",
      "For SimpleRNN: Epoch: 1, Loss: 543.49152\n",
      "For SimpleRNN: Epoch: 2, Loss: 535.11523\n",
      "For SimpleRNN: Epoch: 3, Loss: 527.40399\n",
      "For SimpleRNN: Epoch: 4, Loss: 520.43207\n",
      "For SimpleRNN: Epoch: 5, Loss: 513.68231\n",
      "For SimpleRNN: Epoch: 6, Loss: 506.98502\n",
      "For SimpleRNN: Epoch: 7, Loss: 500.25912\n",
      "For SimpleRNN: Epoch: 8, Loss: 493.30066\n",
      "For SimpleRNN: Epoch: 9, Loss: 485.98740\n",
      "For SimpleRNN: Epoch: 10, Loss: 479.25558\n",
      "For SimpleRNN: Epoch: 11, Loss: 472.62903\n",
      "For SimpleRNN: Epoch: 12, Loss: 465.73642\n",
      "For SimpleRNN: Epoch: 13, Loss: 458.15955\n",
      "For SimpleRNN: Epoch: 14, Loss: 450.66479\n",
      "For SimpleRNN: Epoch: 15, Loss: 443.40033\n",
      "For SimpleRNN: Epoch: 16, Loss: 436.28262\n",
      "For SimpleRNN: Epoch: 17, Loss: 428.77496\n",
      "For SimpleRNN: Epoch: 18, Loss: 420.92102\n",
      "For SimpleRNN: Epoch: 19, Loss: 412.85147\n",
      "For SimpleRNN: Epoch: 20, Loss: 404.82715\n",
      "For SimpleRNN: Epoch: 21, Loss: 396.81866\n",
      "For SimpleRNN: Epoch: 22, Loss: 388.52243\n",
      "For SimpleRNN: Epoch: 23, Loss: 380.09171\n",
      "For SimpleRNN: Epoch: 24, Loss: 371.71252\n",
      "For SimpleRNN: Epoch: 25, Loss: 363.57626\n",
      "For SimpleRNN: Epoch: 26, Loss: 355.22412\n",
      "For SimpleRNN: Epoch: 27, Loss: 347.24963\n",
      "For SimpleRNN: Epoch: 28, Loss: 339.15912\n",
      "For SimpleRNN: Epoch: 29, Loss: 330.95694\n"
     ]
    }
   ],
   "source": [
    "# Generate training data for rho=10, 28, 40\n",
    "training_input = []\n",
    "training_output = []\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "model_rnn = SimpleRNN(3, 50, 3)  # input size = 3, hidden size = 50, output size = 3\n",
    "optimizer = Adam(model_rnn.parameters(), lr=0.01)  # adjust the learning rate if necessary\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(30):  # 30 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_rnn(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "\n",
    "    # Add L2 regularization\n",
    "    l2_reg = None\n",
    "    for W in model_rnn.parameters():\n",
    "        if l2_reg is None:\n",
    "            l2_reg = W.norm(2)\n",
    "        else:\n",
    "            l2_reg = l2_reg + W.norm(2)\n",
    "    loss += 0.01 * l2_reg\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model_rnn.parameters(), max_norm=1)\n",
    "\n",
    "    optimizer.step()    # Does the update\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    print('For SimpleRNN: Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM: rho = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LSTM : Epoch: 0, Loss: 554.13330\n",
      "For LSTM : Epoch: 1, Loss: 553.48798\n",
      "For LSTM : Epoch: 2, Loss: 552.84259\n",
      "For LSTM : Epoch: 3, Loss: 552.19666\n",
      "For LSTM : Epoch: 4, Loss: 551.54950\n",
      "For LSTM : Epoch: 5, Loss: 550.90076\n",
      "For LSTM : Epoch: 6, Loss: 550.24994\n",
      "For LSTM : Epoch: 7, Loss: 549.59662\n",
      "For LSTM : Epoch: 8, Loss: 548.94055\n",
      "For LSTM : Epoch: 9, Loss: 548.28125\n",
      "For LSTM : Epoch: 10, Loss: 547.61853\n",
      "For LSTM : Epoch: 11, Loss: 546.95209\n",
      "For LSTM : Epoch: 12, Loss: 546.28162\n",
      "For LSTM : Epoch: 13, Loss: 545.60669\n",
      "For LSTM : Epoch: 14, Loss: 544.92688\n",
      "For LSTM : Epoch: 15, Loss: 544.24176\n",
      "For LSTM : Epoch: 16, Loss: 543.55084\n",
      "For LSTM : Epoch: 17, Loss: 542.85400\n",
      "For LSTM : Epoch: 18, Loss: 542.15106\n",
      "For LSTM : Epoch: 19, Loss: 541.44232\n",
      "For LSTM : Epoch: 20, Loss: 540.72839\n",
      "For LSTM : Epoch: 21, Loss: 540.01007\n",
      "For LSTM : Epoch: 22, Loss: 539.28857\n",
      "For LSTM : Epoch: 23, Loss: 538.56519\n",
      "For LSTM : Epoch: 24, Loss: 537.84082\n",
      "For LSTM : Epoch: 25, Loss: 537.11621\n",
      "For LSTM : Epoch: 26, Loss: 536.39178\n",
      "For LSTM : Epoch: 27, Loss: 535.66748\n",
      "For LSTM : Epoch: 28, Loss: 534.94287\n",
      "For LSTM : Epoch: 29, Loss: 534.21747\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and optimizer\n",
    "model_LSTM = LSTM(3, 50, 3)  # input size = 3, hidden size = 50, output size = 3\n",
    "optimizer = Adam(model_LSTM.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(30):  # 30 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model_LSTM(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    print('For LSTM : Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ESN : Epoch: 0, Loss: 553.31342\n",
      "For ESN : Epoch: 1, Loss: 542.24335\n",
      "For ESN : Epoch: 2, Loss: 531.35760\n",
      "For ESN : Epoch: 3, Loss: 520.64752\n",
      "For ESN : Epoch: 4, Loss: 510.10434\n",
      "For ESN : Epoch: 5, Loss: 499.71899\n",
      "For ESN : Epoch: 6, Loss: 489.48404\n",
      "For ESN : Epoch: 7, Loss: 479.39423\n",
      "For ESN : Epoch: 8, Loss: 469.44647\n",
      "For ESN : Epoch: 9, Loss: 459.63950\n",
      "For ESN : Epoch: 10, Loss: 449.97397\n",
      "For ESN : Epoch: 11, Loss: 440.45200\n",
      "For ESN : Epoch: 12, Loss: 431.07632\n",
      "For ESN : Epoch: 13, Loss: 421.84982\n",
      "For ESN : Epoch: 14, Loss: 412.77466\n",
      "For ESN : Epoch: 15, Loss: 403.85260\n",
      "For ESN : Epoch: 16, Loss: 395.08456\n",
      "For ESN : Epoch: 17, Loss: 386.47079\n",
      "For ESN : Epoch: 18, Loss: 378.01105\n",
      "For ESN : Epoch: 19, Loss: 369.70471\n",
      "For ESN : Epoch: 20, Loss: 361.55081\n",
      "For ESN : Epoch: 21, Loss: 353.54852\n",
      "For ESN : Epoch: 22, Loss: 345.69717\n",
      "For ESN : Epoch: 23, Loss: 337.99615\n",
      "For ESN : Epoch: 24, Loss: 330.44519\n",
      "For ESN : Epoch: 25, Loss: 323.04407\n",
      "For ESN : Epoch: 26, Loss: 315.79248\n",
      "For ESN : Epoch: 27, Loss: 308.68997\n",
      "For ESN : Epoch: 28, Loss: 301.73599\n",
      "For ESN : Epoch: 29, Loss: 294.92947\n"
     ]
    }
   ],
   "source": [
    "# Define the models\n",
    "model_esn_10 = ESN(3, 50, 3)\n",
    "\n",
    "# Initialize the optimizer for ESN\n",
    "optimizer_esn = torch.optim.Adam(model_esn_10.output_weights.parameters(), lr=0.01)\n",
    "\n",
    "# Add an extra dimension for time step to the input tensor\n",
    "training_input_torch_time = training_input_torch.unsqueeze(1)\n",
    "\n",
    "# Train the ESN network\n",
    "for epoch in range(30):  # match the number of epochs with NN\n",
    "    optimizer_esn.zero_grad()   # zero the gradient buffers\n",
    "    output_esn = model_esn_10(training_input_torch_time)\n",
    "    # Remove the time step dimension from the output for loss calculation\n",
    "    output_esn = output_esn.squeeze(1)\n",
    "    loss_esn = criterion(output_esn, training_output_torch)\n",
    "    loss_esn.backward()\n",
    "    optimizer_esn.step()    # Does the update\n",
    "    print('For ESN : Epoch: {}, Loss: {:.5f}'.format(epoch, loss_esn.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forecasting dynamics for rho = 17:\n",
      "Mean Squared Error for model Net: 7.215717050712556e-05\n",
      "Mean Squared Error for model FeedForwardNN: 4.492899097385816e-05\n",
      "Mean Squared Error for model LSTM: 9.988838428398594e-05\n",
      "Mean Squared Error for model SimpleRNN: 0.0012948585208505392\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ZiwenLi\\Desktop\\UW\\EE\\399\\hw5\\hw5.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     output \u001b[39m=\u001b[39m model(train_input_torch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, train_output_torch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ZiwenLi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\ZiwenLi\\Desktop\\UW\\EE\\399\\hw5\\hw5.ipynb Cell 16\u001b[0m in \u001b[0;36mESN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     reservoir_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreservoir_size, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     reservoir_state[:, \u001b[39m0\u001b[39m, :] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(torch\u001b[39m.\u001b[39mmm(\u001b[39minput\u001b[39;49m[:,\u001b[39m0\u001b[39;49m,:], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_weights\u001b[39m.\u001b[39mt()))  \u001b[39m# initialize reservoir state at t=0\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)):  \u001b[39m# start loop from t=1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X31sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m         reservoir_state[:, t, :] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(torch\u001b[39m.\u001b[39mmm(\u001b[39minput\u001b[39m[:,t,:], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_weights\u001b[39m.\u001b[39mt()) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mmm(reservoir_state[:, t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreservoir_weights\u001b[39m.\u001b[39mt()))\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "rhos = [17, 35]\n",
    "\n",
    "def generate_lorenz_data(rho, initial_state=[1, 1, 1], dt=0.01, N=10000, sigma=10, beta=8/3):\n",
    "    t = np.arange(0, N*dt, dt)\n",
    "    traj = integrate.odeint(lorenz_deriv, initial_state, t, args=(sigma, beta, rho))\n",
    "    return traj[:-1, :], traj[1:, :]\n",
    "\n",
    "for rho in rhos:\n",
    "    print(f\"\\nForecasting dynamics for rho = {rho}:\")\n",
    "\n",
    "    # Generate Lorenz system data for rho\n",
    "    nn_input, nn_output = generate_lorenz_data(rho)\n",
    "\n",
    "    # Train-test split\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(nn_input))\n",
    "    train_input, train_output = nn_input[:split_idx], nn_output[:split_idx]\n",
    "    test_input, test_output = nn_input[split_idx:], nn_output[split_idx:]\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    train_input_torch = torch.from_numpy(train_input.astype(np.float32))\n",
    "    train_output_torch = torch.from_numpy(train_output.astype(np.float32))\n",
    "    test_input_torch = torch.from_numpy(test_input.astype(np.float32))\n",
    "    test_output_torch = torch.from_numpy(test_output.astype(np.float32))\n",
    "\n",
    "    # Define the models\n",
    "    model1 = model_nn_10\n",
    "    model2 = model_FFNN_10\n",
    "    model3 = model_LSTM\n",
    "    model4 = model_rnn\n",
    "    model5 = model_esn_10\n",
    "  \n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train each model and make predictions\n",
    "    for model in [model1, model2, model3, model4, model5]:\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_input_torch)\n",
    "            loss = criterion(output, train_output_torch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(test_input_torch)\n",
    "\n",
    "        # Compute the mean squared error of the predictions\n",
    "        mse = mean_squared_error(test_output_torch.detach().numpy(), predictions.detach().numpy())\n",
    "        print(f\"Mean Squared Error for model {model.__class__.__name__}: {mse}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forecasting dynamics for rho = 17:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ZiwenLi\\Desktop\\UW\\EE\\399\\hw5\\hw5.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X33sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)  \u001b[39m# remove the time dimension for ESN output\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X33sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, train_output_torch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X33sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X33sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ZiwenLi/Desktop/UW/EE/399/hw5/hw5.ipynb#X33sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Make predictions on the test data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ZiwenLi\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ZiwenLi\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "for rho in rhos:\n",
    "    print(f\"\\nForecasting dynamics for rho = {rho}:\")\n",
    "\n",
    "    # Generate Lorenz system data for rho\n",
    "    nn_input, nn_output = generate_lorenz_data(rho)\n",
    "\n",
    "    # Train-test split\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(nn_input))\n",
    "    train_input, train_output = nn_input[:split_idx], nn_output[:split_idx]\n",
    "    test_input, test_output = nn_input[split_idx:], nn_output[split_idx:]\n",
    "\n",
    "    # Convert to torch tensors and add time dimension\n",
    "    train_input_torch = torch.from_numpy(train_input.astype(np.float32)).unsqueeze(1)\n",
    "    train_output_torch = torch.from_numpy(train_output.astype(np.float32))\n",
    "    test_input_torch = torch.from_numpy(test_input.astype(np.float32)).unsqueeze(1)\n",
    "    test_output_torch = torch.from_numpy(test_output.astype(np.float32))\n",
    "\n",
    "      # Define the models\n",
    "\n",
    "    model5 = model_esn_10\n",
    "  \n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train each model and make predictions\n",
    "    for model in [model5]:\n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            output = output.squeeze(1)  # remove the time dimension for ESN output\n",
    "            loss = criterion(output, train_output_torch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = predictions.squeeze(1)  # remove the time dimension for ESN predictions\n",
    "\n",
    "        # Compute the mean squared error of the predictions\n",
    "        mse = mean_squared_error(test_output_torch.detach().numpy(), predictions.detach().numpy())\n",
    "        print(f\"Mean Squared Error for model {model.__class__.__name__}: {mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
