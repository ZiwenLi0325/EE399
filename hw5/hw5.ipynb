{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE399 HW4\n",
    "## Ziwen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ZiwenLi0325/EE399.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from scipy import integrate\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 298.25491\n",
      "Epoch: 10, Loss: 197.73874\n",
      "Epoch: 20, Loss: 115.29742\n",
      "Epoch: 30, Loss: 51.61908\n",
      "Epoch: 40, Loss: 17.40350\n",
      "Epoch: 50, Loss: 6.37015\n",
      "Epoch: 60, Loss: 3.60739\n",
      "Epoch: 70, Loss: 2.96787\n",
      "Epoch: 80, Loss: 2.46544\n",
      "Epoch: 90, Loss: 2.10413\n",
      "Test loss: 1.4272576570510864\n"
     ]
    }
   ],
   "source": [
    "dt = 0.01\n",
    "T = 8\n",
    "t = np.arange(0,T+dt,dt)\n",
    "beta = 8/3\n",
    "sigma = 10\n",
    "rho = 28\n",
    "\n",
    "# Define the Lorenz system\n",
    "def lorenz_deriv(x_y_z, t0, sigma=sigma, beta=beta, rho=rho):\n",
    "    x, y, z = x_y_z\n",
    "    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "\n",
    "# Generate training data for rho=10, 28, 40\n",
    "rhos = [10, 28, 40]\n",
    "training_input = []\n",
    "training_output = []\n",
    "\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        training_input.append(x_t[j,:-1,:])\n",
    "        training_output.append(x_t[j,1:,:])\n",
    "\n",
    "training_input = np.vstack(training_input)\n",
    "training_output = np.vstack(training_output)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "training_input_torch = torch.tensor(training_input, dtype=torch.float32)\n",
    "training_output_torch = torch.tensor(training_output, dtype=torch.float32)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = Net()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n",
    "\n",
    "# Now test for rho=17 and 35\n",
    "rhos = [17, 35]\n",
    "testing_input = []\n",
    "testing_output = []\n",
    "\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        testing_input.append(x_t[j,:-1,:])\n",
    "        testing_output.append(x_t[j,1:,:])\n",
    "\n",
    "testing_input = np.vstack(testing_input)\n",
    "testing_output = np.vstack(testing_output)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "testing_input_torch = torch.tensor(testing_input, dtype=torch.float32)\n",
    "testing_output_torch = torch.tensor(testing_output, dtype=torch.float32)\n",
    "\n",
    "# Predict future states\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not calculate gradients\n",
    "    predictions = model(testing_input_torch)\n",
    "\n",
    "# Calculate the loss between the predictions and the true future states\n",
    "test_loss = criterion(predictions, testing_output_torch)\n",
    "print('Test loss:', test_loss.item())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 279.39880\n",
      "Epoch: 10, Loss: 184.66254\n",
      "Epoch: 20, Loss: 103.02059\n",
      "Epoch: 30, Loss: 45.08611\n",
      "Epoch: 40, Loss: 21.29359\n",
      "Epoch: 50, Loss: 10.91245\n",
      "Epoch: 60, Loss: 4.06014\n",
      "Epoch: 70, Loss: 2.64286\n",
      "Epoch: 80, Loss: 1.86473\n",
      "Epoch: 90, Loss: 1.21212\n",
      "Test loss: 0.6688587069511414\n"
     ]
    }
   ],
   "source": [
    "# Define the Lorenz system\n",
    "def lorenz_deriv(x_y_z, t0, sigma=sigma, beta=beta, rho=rho):\n",
    "    x, y, z = x_y_z\n",
    "    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "\n",
    "# Generate training data for rho=10, 28, 40\n",
    "rhos = [10, 28, 40]\n",
    "training_input = []\n",
    "training_output = []\n",
    "\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        training_input.append(x_t[j,:-1,:])\n",
    "        training_output.append(x_t[j,1:,:])\n",
    "\n",
    "training_input = np.vstack(training_input)\n",
    "training_output = np.vstack(training_output)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "training_input_torch = torch.tensor(training_input, dtype=torch.float32)\n",
    "training_output_torch = torch.tensor(training_output, dtype=torch.float32)\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 50)  # Input dimension is 3 (for x, y, z)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 3)  # Output dimension is 3 (for x, y, z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # No activation function here because the output can be any real number\n",
    "    \n",
    "# Initialize the model and optimizer\n",
    "model = Net()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n",
    "\n",
    "# Now test for rho=17 and 35\n",
    "rhos = [17, 35]\n",
    "testing_input = []\n",
    "testing_output = []\n",
    "\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        testing_input.append(x_t[j,:-1,:])\n",
    "        testing_output.append(x_t[j,1:,:])\n",
    "\n",
    "testing_input = np.vstack(testing_input)\n",
    "testing_output = np.vstack(testing_output)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "testing_input_torch = torch.tensor(testing_input, dtype=torch.float32)\n",
    "testing_output_torch = torch.tensor(testing_output, dtype=torch.float32)\n",
    "\n",
    "# Predict future states\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not calculate gradients\n",
    "    predictions = model(testing_input_torch)\n",
    "\n",
    "# Calculate the loss between the predictions and the true future states\n",
    "test_loss = criterion(predictions, testing_output_torch)\n",
    "print('Test loss:', test_loss.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 310.94574\n",
      "Epoch: 10, Loss: 232.57372\n",
      "Epoch: 20, Loss: 159.25790\n",
      "Epoch: 30, Loss: 84.63853\n",
      "Epoch: 40, Loss: 28.08471\n",
      "Epoch: 50, Loss: 8.21618\n",
      "Epoch: 60, Loss: 5.30645\n",
      "Epoch: 70, Loss: 2.64519\n",
      "Epoch: 80, Loss: 1.94134\n",
      "Epoch: 90, Loss: 1.25726\n",
      "Test loss: 0.6699401140213013\n"
     ]
    }
   ],
   "source": [
    "# Define the Lorenz system\n",
    "def lorenz_deriv(x_y_z, t0, sigma=sigma, beta=beta, rho=rho):\n",
    "    x, y, z = x_y_z\n",
    "    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "\n",
    "# Generate training data for rho=10, 28, 40\n",
    "rhos = [10, 28, 40]\n",
    "training_input = []\n",
    "training_output = []\n",
    "\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        training_input.append(x_t[j,:-1,:])\n",
    "        training_output.append(x_t[j,1:,:])\n",
    "\n",
    "training_input = np.vstack(training_input)\n",
    "training_output = np.vstack(training_output)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "training_input_torch = torch.tensor(training_input, dtype=torch.float32)\n",
    "training_output_torch = torch.tensor(training_output, dtype=torch.float32)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 50)  # Input dimension is 3 (for x, y, z)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 3)  # Output dimension is 3 (for x, y, z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # No activation function here because the output can be any real number\n",
    "    \n",
    "# Initialize the model and optimizer\n",
    "model = Net()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n",
    "\n",
    "# Now test for rho=17 and 35\n",
    "rhos = [17, 35]\n",
    "testing_input = []\n",
    "testing_output = []\n",
    "\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        testing_input.append(x_t[j,:-1,:])\n",
    "        testing_output.append(x_t[j,1:,:])\n",
    "\n",
    "testing_input = np.vstack(testing_input)\n",
    "testing_output = np.vstack(testing_output)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "testing_input_torch = torch.tensor(testing_input, dtype=torch.float32)\n",
    "testing_output_torch = torch.tensor(testing_output, dtype=torch.float32)\n",
    "\n",
    "# Predict future states\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not calculate gradients\n",
    "    predictions = model(testing_input_torch)\n",
    "\n",
    "# Calculate the loss between the predictions and the true future states\n",
    "test_loss = criterion(predictions, testing_output_torch)\n",
    "print('Test loss:', test_loss.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 330.02725\n",
      "Epoch: 10, Loss: 257.17392\n",
      "Epoch: 20, Loss: 200.10478\n",
      "Epoch: 30, Loss: 138.83022\n",
      "Epoch: 40, Loss: 76.49510\n",
      "Epoch: 50, Loss: 27.27403\n",
      "Epoch: 60, Loss: 4.38100\n",
      "Epoch: 70, Loss: 2.60705\n",
      "Epoch: 80, Loss: 1.35862\n",
      "Epoch: 90, Loss: 0.90163\n",
      "Test loss: 0.5337609648704529\n"
     ]
    }
   ],
   "source": [
    "# Define the Lorenz system\n",
    "def lorenz_deriv(x_y_z, t0, sigma=sigma, beta=beta, rho=rho):\n",
    "    x, y, z = x_y_z\n",
    "    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "\n",
    "# Generate training data for rho=10, 28, 40\n",
    "rhos = [10, 28, 40]\n",
    "training_input = []\n",
    "training_output = []\n",
    "\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        training_input.append(x_t[j,:-1,:])\n",
    "        training_output.append(x_t[j,1:,:])\n",
    "\n",
    "training_input = np.vstack(training_input)\n",
    "training_output = np.vstack(training_output)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "training_input_torch = torch.tensor(training_input, dtype=torch.float32)\n",
    "training_output_torch = torch.tensor(training_output, dtype=torch.float32)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=3, hidden_size=20, num_layers=1)\n",
    "        self.fc = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        return self.fc(x[-1, :, :])\n",
    "    \n",
    "# Initialize the model and optimizer\n",
    "model = Net()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = model(training_input_torch)\n",
    "    loss = criterion(output, training_output_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n",
    "\n",
    "# Now test for rho=17 and 35\n",
    "rhos = [17, 35]\n",
    "testing_input = []\n",
    "testing_output = []\n",
    "\n",
    "for rho in rhos:\n",
    "    np.random.seed(123)\n",
    "    x0 = -15 + 30 * np.random.random((100, 3))\n",
    "\n",
    "    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(sigma, beta, rho)) for x0_j in x0])\n",
    "    \n",
    "    for j in range(100):\n",
    "        testing_input.append(x_t[j,:-1,:])\n",
    "        testing_output.append(x_t[j,1:,:])\n",
    "\n",
    "testing_input = np.vstack(testing_input)\n",
    "testing_output = np.vstack(testing_output)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "testing_input_torch = torch.tensor(testing_input, dtype=torch.float32)\n",
    "testing_output_torch = torch.tensor(testing_output, dtype=torch.float32)\n",
    "\n",
    "# Predict future states\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not calculate gradients\n",
    "    predictions = model(testing_input_torch)\n",
    "\n",
    "# Calculate the loss between the predictions and the true future states\n",
    "test_loss = criterion(predictions, testing_output_torch)\n",
    "print('Test loss:', test_loss.item())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
