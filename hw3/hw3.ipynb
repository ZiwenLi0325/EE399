{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE399 HW3\n",
    "## Ziwen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ZiwenLi0325/EE399.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load MNIST data set\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target.astype('int64')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD analysis\n",
    "U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "print(len(S))\n",
    "plt.plot(np.cumsum(S**2) / np.sum(S**2))\n",
    "plt.xlabel('Number of singular values')\n",
    "plt.ylabel('Cumulative variance explained')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(S)\n",
    "plt.xlabel('Singular value index')\n",
    "plt.ylabel('Singular value')\n",
    "plt.text(0.5, -0.1 ,'Figure 1: Singular value distribution. X-axis is the index of corresponding singular value from SVD.', ha='center', fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.title(\"singular_value_spectrum\")\n",
    "plt.gcf().set_size_inches(12, 8)\n",
    "plt.savefig(\"singular_val.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_var = np.cumsum(S**2) / np.sum(S**2)\n",
    "plt.plot(cumulative_var[:50])\n",
    "plt.xlabel('Number of singular values')\n",
    "plt.ylabel('Cumulative variance explained')\n",
    "plt.text(0.5, -0.1 ,'Figure 2: Variance of Singular value distribution. ', ha='center', fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.title(\"Variance of Singular Value\")\n",
    "plt.gcf().set_size_inches(12, 8)\n",
    "plt.savefig(\"singular_val_var.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-10\n",
    "rank = np.sum(S > threshold)\n",
    "print(rank)\n",
    "print(np.linalg.matrix_rank(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select three V-modes\n",
    "V_modes = U[:, [2, 3, 5]]\n",
    "\n",
    "# Project the digit images onto the selected V-modes\n",
    "proj = np.array(X.T @ V_modes)\n",
    "\n",
    "# Create a 3D plot with digit labels as colors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(proj.shape[0]):\n",
    "    x, y, z = proj[i, :]\n",
    "    ax.scatter(x, y, z, cmap='viridis')\n",
    "ax.set_xlabel('V-mode 2')\n",
    "ax.set_ylabel('V-mode 3')\n",
    "ax.set_zlabel('V-mode 5')\n",
    "plt.title(\"3D plot\")\n",
    "plt.gcf().set_size_inches(12, 8)\n",
    "ax.text2D(0, -0.1, \"Figure 3: 3D plot for projection onto three selected V-modes (columns) colored by their digit label at columns 2,3, and 5.,\", transform=ax.transAxes)\n",
    "plt.savefig(\"3D_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (input) and labels (output) from data set\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# Convert labels to integers\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "# Divide the data set into training and test sets\n",
    "X_train, x_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "# Select the digits to classify\n",
    "digit1 = 1\n",
    "digit2 = 7\n",
    "\n",
    "# Extract the corresponding images and labels\n",
    "mask = (y_train == digit1) | (y_train == digit2)\n",
    "X = X_train[mask]\n",
    "y = y_train[mask]\n",
    "y = (y == digit1).astype(int)  # 1 for digit1, 0 for digit2\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform SVD on the training data to reduce its dimensionality\n",
    "U, s, Vt = np.linalg.svd(X_train, full_matrices=False)\n",
    "\n",
    "\n",
    "# Compute the LDA projection matrix using the reduced training data\n",
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "W = lda.coef_.T\n",
    "\n",
    "# Project the reduced training and testing data onto the LDA subspace\n",
    "X_train_lda = X_train @ W\n",
    "X_test_lda = X_test @ W\n",
    "\n",
    "# Train a logistic regression model on the LDA-projected training data\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train_lda, y_train)\n",
    "\n",
    "# Evaluate the model's performance on the LDA-projected testing data\n",
    "accuracy = clf.score(X_test_lda, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {ii}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data and labels\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "# Choose three digits to classify\n",
    "digits = [2, 3, 8]\n",
    "\n",
    "# Filter the data and labels to only include the selected digits\n",
    "X_filtered = X[np.isin(y, digits)]\n",
    "y_filtered = y[np.isin(y, digits)]\n",
    "\n",
    "# Split the filtered data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reduce the dimensionality of the data using SVD\n",
    "U, s, Vt = np.linalg.svd(X_train, full_matrices=False)\n",
    "\n",
    "# Compute the LDA projection matrix using the reduced training data\n",
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "W = lda.coef_\n",
    "\n",
    "# Project the test data onto the LDA projection matrix\n",
    "X_test_lda = X_test @ W.T\n",
    "\n",
    "# Compute the accuracy of the classifier on the test data\n",
    "y_pred = lda.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### {iii}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the digits to classify\n",
    "digit1 = '4'\n",
    "digit2 = '9'\n",
    "\n",
    "# Create a binary classification dataset for the two digits\n",
    "X = mnist.data[(mnist.target == digit1) | (mnist.target == digit2)]\n",
    "y = mnist.target[(mnist.target == digit1) | (mnist.target == digit2)]\n",
    "y = np.array([int(d == digit2) for d in y])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an LDA object and fit it to the training data\n",
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "# Compute the accuracy for each digit separately\n",
    "acc_digit1 = accuracy_score(y_test[y_test==0], y_pred[y_test==0])\n",
    "acc_digit2 = accuracy_score(y_test[y_test==1], y_pred[y_test==1])\n",
    "\n",
    "print(f\"Accuracy for digit {digit1}: {acc_digit1}\")\n",
    "print(f\"Accuracy for digit {digit2}: {acc_digit2}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the digits to classify\n",
    "digit1 = '0'\n",
    "digit2 = '5'\n",
    "\n",
    "# Create a binary classification dataset for the two digits\n",
    "X = mnist.data[(mnist.target == digit1) | (mnist.target == digit2)]\n",
    "y = mnist.target[(mnist.target == digit1) | (mnist.target == digit2)]\n",
    "y = np.array([int(d == digit2) for d in y])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an LDA object and fit it to the training data\n",
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "# Compute the accuracy for each digit separately\n",
    "acc_digit1 = accuracy_score(y_test[y_test==0], y_pred[y_test==0])\n",
    "acc_digit2 = accuracy_score(y_test[y_test==1], y_pred[y_test==1])\n",
    "\n",
    "print(f\"Accuracy for digit {digit1}: {acc_digit1}\")\n",
    "print(f\"Accuracy for digit {digit2}: {acc_digit2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = mnist.data\n",
    "y = mnist.target.astype(int)\n",
    "\n",
    "# Choose digit 0 and digit 5\n",
    "digits = [0, 5]\n",
    "idx = np.isin(y, digits)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit LDA to the training data\n",
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy for separating digit 0 and digit 5: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classfier(X,y,i,j):\n",
    "    # Define X and y\n",
    "   \n",
    "    digits = [i, j]\n",
    "    idx = np.isin(y, digits)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "    # Fit LDA to the training data\n",
    "    lda = LDA()\n",
    "    lda.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = lda.predict(X_test)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    #print(\"Accuracy for separating digit \"+ str(i) +\"  and digit \" + str(j) +\" : {:.3f}\".format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_matrix = np.zeros([10,10])\n",
    "X = mnist.data\n",
    "y = mnist.target.astype(int)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        accuracy_matrix[i,j] = classfier(X,y,i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(accuracy_matrix)\n",
    "plt.colorbar()\n",
    "min_idx = np.unravel_index(np.argmin(accuracy_matrix), accuracy_matrix.shape)\n",
    "# print the result\n",
    "print(min_idx)\n",
    "plt.title(\"Classification probability Matrix\")\n",
    "plt.gcf().set_size_inches(12, 8)\n",
    "plt.text(0.5, -0.1 ,'Figure 4: Classification probability Matrix. The higher the probability, the eaiser to distinguish between two digits. ', ha='center', fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.savefig(\"Classification.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index i,j with the smallest entry\n",
    "large = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if accuracy_matrix[i,j] > large and i!=j:\n",
    "            large = accuracy_matrix[i,j] \n",
    "            i_index = i\n",
    "            j_index = j\n",
    "print(f\"The index i,j with the smallest entry is ({i_index}, {j_index})\")\n",
    "print(large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_matrix[i_index,j_index])\n",
    "print(accuracy_matrix[min_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of all images with the digits 0 and 1\n",
    "idx_01 = np.where((y == 0) | (y == 1))[0]\n",
    "X_01, y_01 = np.array(X)[idx_01], np.array(y)[idx_01]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_01, X_test_01, y_train_01, y_test_01 = train_test_split(X_01, y_01, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LDA classifier\n",
    "lda = LDA()\n",
    "lda.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Train SVM classifier\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "svm.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Train decision tree classifier\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "dtc.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Test the classifiers on the test set\n",
    "y_pred_lda = lda.predict(X_test_01)\n",
    "y_pred_svm = svm.predict(X_test_01)\n",
    "y_pred_dtc = dtc.predict(X_test_01)\n",
    "\n",
    "# Calculate the accuracy scores\n",
    "accuracy_lda = accuracy_score(y_test_01, y_pred_lda)\n",
    "accuracy_svm = accuracy_score(y_test_01, y_pred_svm)\n",
    "accuracy_dtc = accuracy_score(y_test_01, y_pred_dtc)\n",
    "\n",
    "print(\"Accuracy scores for LDA, SVM, and decision tree classifiers on digits 0 vs 1:\")\n",
    "print(\"LDA: {:.2f}%\".format(accuracy_lda * 100))\n",
    "print(\"SVM: {:.2f}%\".format(accuracy_svm * 100))\n",
    "print(\"Decision Tree: {:.2f}%\".format(accuracy_dtc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
